{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** This notebook uses the cropped images of vehicles extracted from the original aerial imagery by running the Image Pre-Processing notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import math\n",
    "\n",
    "from collections import Counter\n",
    "from keras import backend as K\n",
    "from keras import models, layers, optimizers, losses, applications, callbacks, metrics\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the network using MobileNet as base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNet(models.Sequential):\n",
    "    \"\"\"\n",
    "        Using pre-trained MobileNet without head\n",
    "    \"\"\"\n",
    "    def __init__(self, outputs=37, output_layer='sigmoid', **kwargs):\n",
    "        super(MNet, self).__init__()\n",
    "\n",
    "        sizes = [(128,128,3), (4,4,1024), 1024, outputs]\n",
    "\n",
    "        # Load the pre-trained base model\n",
    "        base = applications.MobileNet(\n",
    "            weights='imagenet',\n",
    "            include_top=False,\n",
    "            input_shape=sizes[0])\n",
    "\n",
    "        self.add(base)\n",
    "        self.add(layers.Flatten())\n",
    "        self.add(layers.Dense(sizes[-2], activation='relu'))\n",
    "        self.add(layers.Dense(sizes[-1], activation=output_layer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define image data generators for training, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30330 images belonging to 37 classes.\n",
      "Found 7565 images belonging to 37 classes.\n",
      "Found 11879 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "source_dir = './dataset_v2/train/classes/'\n",
    "category = ''\n",
    "\n",
    "generator_args = {\n",
    "    'validation_split':0.2,\n",
    "    'rotation_range':45,\n",
    "    'width_shift_range':0.2,\n",
    "    'height_shift_range':0.2,\n",
    "    'horizontal_flip':True,\n",
    "    'vertical_flip':True,\n",
    "    'fill_mode':'constant',\n",
    "    'cval':255    \n",
    "}\n",
    "generator = ImageDataGenerator(**generator_args)\n",
    "        \n",
    "flow_args = {\n",
    "    'target_size':(128, 128),\n",
    "    'batch_size':32,\n",
    "    'directory': f'{source_dir}{category}',\n",
    "    # 'save_to_dir':'../dataset_v2/train/saved'\n",
    "}\n",
    "\n",
    "train_gen = generator.flow_from_directory(subset='training', **flow_args)\n",
    "valid_gen = generator.flow_from_directory(subset='validation', **flow_args)\n",
    "test_gen = ImageDataGenerator().flow_from_directory(\n",
    "    directory='./dataset_v2/test/', \n",
    "    shuffle=False, \n",
    "    target_size=(128, 128), \n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define weighted cross-entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_weight(generator):\n",
    "    Nm = np.array(sorted(Counter(generator.classes).items()))[:,1]\n",
    "    w0 = np.maximum(Nm.astype(np.float)/generator.samples, 0.1)\n",
    "    return (1 - w0) / w0\n",
    "\n",
    "w = get_class_weight(train_gen)\n",
    "_EPSILON = K.epsilon()\n",
    "\n",
    "def weighted_crossentropy(y_true, y_pred):\n",
    "    y_pred = K.clip(y_pred, _EPSILON, 1.0-_EPSILON)\n",
    "    out = -(K.constant(w) * y_true * K.log(y_pred) + (1.0 - y_true) * K.log(1.0 - y_pred))\n",
    "    return K.mean(out, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the model and training parameters and fit to the training data.\n",
    "\n",
    "- The model arguments allows for specifying the number of outputs and output layer. Doing so, the defined model can be used for mutually exclusive features as an extension of the baseline solution.\n",
    "- The compile arguments sets the optimizer and loss function, changing between binary and weighted cross-entropy.\n",
    "- The fit generator arguments sets the data generators as well as the number of training epochs and callbacks to be used during training. The callbacks include an early stopping criteria and a checkpoint saver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "mobilenet_1.00_128 (Model)   (None, 4, 4, 1024)        3228864   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              16778240  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 37)                37925     \n",
      "=================================================================\n",
      "Total params: 20,045,029\n",
      "Trainable params: 20,023,141\n",
      "Non-trainable params: 21,888\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_args = {\n",
    "    # 'outputs':len(train_gen.class_indices),\n",
    "    # 'output_layer':'softmax'\n",
    "}\n",
    "\n",
    "compile_args = {\n",
    "    'optimizer':optimizers.SGD(\n",
    "        lr=0.002, \n",
    "        momentum=0.9, \n",
    "        decay=0.00),\n",
    "    'loss':losses.binary_crossentropy,\n",
    "#     'loss':weighted_crossentropy,\n",
    "    'metrics':['acc']\n",
    "}\n",
    "\n",
    "fit_gen_args = {\n",
    "    'generator':train_gen,\n",
    "    'validation_data':valid_gen,\n",
    "    'steps_per_epoch':len(train_gen),\n",
    "    'validation_steps':len(valid_gen),\n",
    "    'epochs':50,\n",
    "#     'class_weight':dict(enumerate(w)),\n",
    "    'callbacks':[\n",
    "        callbacks.EarlyStopping(\n",
    "            monitor='val_loss', \n",
    "            min_delta=1e-4, \n",
    "            patience=10, \n",
    "            restore_best_weights=True),\n",
    "        # callbacks.ModelCheckpoint(\n",
    "        #     filepath='../models/divided/checkpoint.hdf5', \n",
    "        #     monitor='val_acc', \n",
    "        #     save_weights_only=True,\n",
    "        #     save_best_only=True)\n",
    "    ],\n",
    "}\n",
    "\n",
    "# model_path = '../models/mnet50aug-wce-2.h5'\n",
    "model = MNet(**model_args)    \n",
    "model.summary()\n",
    "model.compile(**compile_args)\n",
    "# model.load_weights(model_path)\n",
    "model.fit_generator(**fit_gen_args)\n",
    "# model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for creating a submission in accordance with the competition template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_index(dataset='test'):\n",
    "    df = pd.read_csv(f'./dataset_v2/{dataset}.csv')\n",
    "    df = df.groupby(['image_id', 'tag_id']).first()\n",
    "    df = df.reset_index('image_id', drop=True)\n",
    "    return df.index\n",
    "\n",
    "def get_pred_columns():\n",
    "    df = pd.read_csv('./dataset_v2/train.csv')\n",
    "    df = df.drop(df.columns[:10], axis=1)\n",
    "    for category in df.columns[df.dtypes == object]:\n",
    "        df[category] = df[category].astype('category')\n",
    "    df = pd.get_dummies(df, prefix='', prefix_sep='')\n",
    "    return sorted(df.columns)\n",
    "\n",
    "def create_answers_by_columns(proba, cols):\n",
    "    pred = pd.DataFrame(proba, index=get_pred_index(), columns=cols)\n",
    "\n",
    "    answers = pd.read_csv('../dataset_v2/answer_template.csv')\n",
    "    for col in pred.columns:\n",
    "        answers[col] = pred[col].sort_values(ascending=False).index\n",
    "\n",
    "    for col in set(answers.columns) - set(pred.columns):\n",
    "        answers[col] = answers[pred.columns[0]]\n",
    "\n",
    "    answers.to_csv('../dataset_v2/answer.csv', index=False)\n",
    "\n",
    "def create_answers(proba):\n",
    "    predictions = pd.DataFrame(proba,\n",
    "                        index=get_pred_index(), \n",
    "                        columns=get_pred_columns())\n",
    "\n",
    "    answers = pd.read_csv('./dataset_v2/answer_template.csv')\n",
    "    for column in answers.columns:\n",
    "        answers[column] = predictions[column].sort_values(ascending=False).index\n",
    "\n",
    "    answers.to_csv('./dataset_v2/answer.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the trained model to predict answers for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = model.predict_generator(test_gen, steps=len(test_gen))\n",
    "create_answers(proba)\n",
    "\n",
    "# In case of predicting a subset of features\n",
    "# cols = os.listdir(f'{source_dir}{category}')\n",
    "\n",
    "# Some additional string manipulation may be required to \n",
    "# conform with the column names used in template\n",
    "# cols = [x.replace('_', ' ') for x in cols]\n",
    "\n",
    "# create_answers_by_columns(proba, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a minimal extraction of the original source code to reproduce the results. \n",
    "\n",
    "Refer to the source directory for the full listing, including\n",
    "- AP and MAP scoring functions for validation data\n",
    "- A model based on ResNet50\n",
    "- Functions for training multiple classifiers\n",
    "- Plotting tools\n",
    "\n",
    "and more."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
